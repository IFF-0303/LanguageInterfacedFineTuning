<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks</title>
  <link rel="icon" type="image/x-icon" href="static/images/lift_logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bar_plot.css">
  <link rel="stylesheet" href="static/css/followup_work.css">
  <link rel="stylesheet" href="static/css/my_slider.css">
  
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script> 
  <script src="static/js/heatmap.js"></script>
  <script defer src="static/js/bar_plot.js"></script>
  <script defer src="static/js/my_slider.js"></script>
  <script defer src="static/js/line_plot.js"></script>
</head>
<body>

  <nav class="nav-wrapper">
    <div class="nav-container">
        <ul class="nav-links">
            <li><a href="#abstract">Abstract</a></li>
            <li><a href="#tasks">Tasks</a></li>
            <li><a href="#properties">Properties</a></li>
            <li><a href="#improvements">Improvements</a></li>
            <li><a href="#spotlights">Spotlights</a></li>
            <li><a href="#BibTeX">Cite Us</a></li>
        </ul>
    </div>
</nav>

  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tuanqdinh.com/" target="_blank">Tuan Dinh</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://yzeng58.github.io" target="_blank">Yuchen Zeng</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://ruisu516.github.io/" target="_blank">Ruisu Zhang</a>,</span>
                  <span class="author-block">
                    <a href="https://myhakureimu.github.io/" target="_blank">Ziqian Lin</a>,</span>
                  <span class="author-block">
                    <a href="https://gira.dev/" target="_blank">Michael Gira</a>,</span>
                  <span class="author-block">
                    <a href="https://shashankrajput.github.io/" target="_blank">Shashank Rajput</a>,</span>
                  <span class="author-block">
                    <a href="https://itml.yonsei.ac.kr/professor" target="_blank">Jy-yong Sohn</a>,</span>
                  <span class="author-block">
                    <a href="https://papail.io/" target="_blank">Dimitris Papailiopoulos</a>,</span>
                  <span class="author-block">
                    <a href="https://kangwooklee.com/aboutme/" target="_blank">Kangwook Lee</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Wisconsin-Madison<br>NeurIPS 2022</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Denotes equal contribution. Author order is alphabetical.</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2206.06565" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://neurips.cc/virtual/2022/poster/54500" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-video"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser" style="margin-top: -3rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/lift_demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        A high-level illustration of the LIFT framework. 
        LIFT has four steps: (i) converting the dataset into sentences, (ii) fine-tuning the pretrained LLMs (e.g., GPT) on the obtained sentences, (iii) generating predictions, and (iv) converting the predictions back to the original data format. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->


<!-- Paper abstract -->
<section class="section hero is-light" id="abstract">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-2">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling "no-code machine learning with LMs." We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->

<!-- Image carousel -->
<section class="hero is-small" id="tasks">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-2">One Model, Many Tasks</h2>

  <!-- Navigation dots -->
  <div class="carousel-nav">
    <div class="nav-buttons">
      <button class="nav-button active">Tabular Regression</button>
      <button class="nav-button">Tabular Classification</button>
      <button class="nav-button">Image Classification</button>
      <button class="nav-button">Image Generation</button>
    </div>
  </div>
      <div class="carousel-container">
        <div class="carousel-track">
          
          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3 has-text-centered" style="margin-top: 1rem;">
                LIFT for Tabular Regression
              </h2>
              <img src="static/images/tabular_regression.png" alt="MY ALT TEXT"/>
              Approximating various functions with LIFT using GPT-J. We visualize the target functions (first row) and the predictor functions learned by LIFT on GPT-J (second row). Blue dots show the 1000 training samples. One can observe that LIFT well approximates the target functions.
            </div>
          </div>

          <div class="carousel-slide">
            <h2 class="title is-3 has-text-centered" style="margin-top: 1rem;">
              LIFT for Tabular Classification
            </h2>
            <div class="heatmap-container" id="heatmapContainer">
              <!-- Heatmap will be inserted here -->
            </div>
            <div class="legend">
              <span>0%</span>
              <div class="legend-gradient"></div>
              <span>100%</span>
            </div>
            <p class="content">
              Accuracies (↑) on OpenML tabular classification datasets. 
              We evaluate LIFT/GPTs on OpenML tabular datasets, varying number of features and data classes. 
              Overall, LIFT/GPTs perform comparably well across tasks, and achieve co mpetitive performances with the best methods, e.g., XGBoost.
            </p>
          </div>

          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3 has-text-centered" style="margin-top: 1rem;">
                LIFT for Image Classification
              </h2>
              <img src="static/images/image_classification.png" alt="MY ALT TEXT"/>
              <div class="heatmap-container" id="imageHeatmapContainer">
                <!-- Image data heatmap will be inserted here -->
              </div>
              <div class="legend">
                <span>0%</span>
                <div class="legend-gradient"></div>
                <span>100%</span>
              </div>
              <p class="content">
                Top: Illustration of using LIFT for image classification.
                Bottom: Accuracies (↑) on image classification datasets. LIFT/GPTs achieve competitive accuracies on this task.
              </p>
            </div>
          </div>

          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3 has-text-centered" style="margin-top: 1rem;">
                LIFT for Image Generation
              </h2>
              <img src="static/images/image_generation.png" alt="MY ALT TEXT"/>
              <p class="content">
                Top: Illustration of using LIFT for image generation.
                Bottom: Generating MNIST images using
                LIFT/GPTs. We observe that LIFT/GPTs can
                generate images of comparably high quality. 
              </p>
            </div>
          </div>
        </div>
      </div>
  
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small is-light" id="properties">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-2">Understanding LIFT: A Deep Dive</h2>

  <!-- Navigation dots -->
  <div class="carousel-nav">
    <div class="nav-buttons">
      <button class="nav-button active">Fine-tuning vs. ICL</button>
      <button class="nav-button">Pretraining Effect</button>
      <button class="nav-button">Robustness</button>
      <button class="nav-button">Benefits of Context</button>
    </div>
  </div>
      <div class="carousel-container">
        <div class="carousel-track">
          
          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3">Language-Interfaced Learning: Fine-Tuning versus In-Context Learning</h2>
      
              <canvas id="liicl"></canvas>
              
              
              <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
                Comparison of accuracies (↑) between Language-Interfaced ICL (LIICL) and Language-Interfaced Fine-Tuning (LIFT) on OpenML datasets.
              </h2>
              <p>
                Due to the context length limit of LMs, we use a subset of the training data for ICL. Here, the size of the subset is chosen to satisfy the LMs' context length. Overall, LIFT/GPTs on full data achieve the best performances. However, when using the same number of samples, LIFT and ICL are more comparable in most cases. Note that both methods may be worse than Majority Classifier due to the limited training data in some cases.
              </p>
            </div>
          </div>

          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
                <h2 class="title is-3">Yes, LIFT Needs Pretrained Knowledge on Natural Language Data</h2>
                <canvas id="modelComparisonChart"></canvas>
                <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
                    Comparison of accuracies (↑) between different models.
                </h2>
                <p>
                  Accuracies (↑) of LIFT with different LMs. We compare variants of LIFT with different
                  LMs: LIFT/GPTs using GPTs pretrained on natural language data (our models), LIFT/Rand-GPT-J
                  using a randomly initialized GPT-J, LIFT/CodeGen and LIFT/CodeParrot using LMs pretrained on
                  programming language data, and LIFT/Gibberish using GPT-J fine-tuned on gibberish data.
                </p>
            </div>
        </div>
          
          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3">Robustness to Feature Corruption on Test Data</h2>
              <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; width: 100%; max-width: 1200px;">
                <div style="width: 100%;">
                  <canvas id="pgdChart1"></canvas>
                </div>
                <div style="width: 100%;">
                  <canvas id="pgdChart2"></canvas>
                </div>
                <div style="width: 100%;">
                  <canvas id="pgdChart3"></canvas>
                </div>
                <div style="width: 100%;">
                  <canvas id="pgdChart4"></canvas>
                </div>
              </div>
              <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
                Results of PGD Attack on Different Models and Attack Strength.
              </h2>
              <p>
                Robustness results on MNIST classification under PGD attacks transferred from LeNet-5 with varying perturbation radius ε. 
                LIFT/GPT-3 cannot tolerate transferred adversarial attack, implying that the adversarial attack on
                LeNet-5 is transferred to LIFT/GPT-3.
              </p>
            </div>
          </div>

          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3">Effect of Context-Aware Prompting</h2>
      
              <canvas id="context"></canvas>
              
              
              <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
                Comparison of accuracies (↑) between models with and without correct feature names on OpenML datasets.
              </h2>
              <p>
                The effect of using feature names on LIFT. We compare classification accuracy (↑) of
                LIFT/GPT-3 when feature names provided in the target dataset are and are not incorporated into
                the prompts. We provide four versions of LIFT when feature names are correctly incorporated
                (Correct-Names columns) and when feature names are randomly shuffled (Shuffled-Names columns).
                We evaluate models on three OpenML datasets, including CMC, TAE, and Vehicle. 
                We also compare our models with the majority classifier. As a result, all LIFT models achieve better performance than the majority classifier. 
                Among the evaluated models, LIFTs with correct feature names achieve the best accuracies on all three datasets.
              </p>
            </div>
          </div>

        </div>
      </div>
  
    </div>
  </div>
</section>

<!-- Image carousel -->
<section class="hero is-small" id="improvements">
  <div class="hero-body">
    <div class="container">

      <h2 class="title is-2">Beyond Basics: Improvements</h2>

  <!-- Navigation dots -->
  <div class="carousel-nav">
    <div class="nav-buttons">
      <button class="nav-button active">Two-Stage Fine-Tuning</button>
      <button class="nav-button">Data Augmentation</button>
    </div>
  </div>
      <div class="carousel-container">
        <div class="carousel-track">
          
          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3">Two-Stage Fine-Tuning</h2>
          
              <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; width: 100%; max-width: 1200px;">
                <div style="width: 100%;">
                  <canvas id="blobsChart"></canvas>
                </div>
                <div style="width: 100%;">
                  <canvas id="openMLChart"></canvas>
                </div>
                <!-- <div style="width: 100%;">
                  <canvas id="regressionChart"></canvas>
                </div> -->
              </div>
          
              <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
                Accuracies (↑) of LIFT/GPT-J with/without two-stage fine-tuning on tabular datasets.
              </h2>
              <p class="content">
                For any given dataset, we first generate two pretext tasks with simple synthetic Gaussian datasets sharing the same number of features and the label space
                (for classification tasks) or the range of responses’ values (for the regression tasks) to the actual
                data. We apply LIFT first on synthetic pretext data before the real datasets, outperforming
                fine-tuning when training data is small.
              </p>
            </div>
          </div>

          <div class="carousel-slide">
            <div style="display: flex; flex-direction: column; justify-content: center; align-items: center; height: 100%;">
              <h2 class="title is-3">Improving Robustness via Data Augmentation</h2>
      
              <div style="display: grid; grid-template-columns: 1fr 1fr; gap: 20px; width: 100%; max-width: 1200px;">
                <div style="width: 100%;">
                  <canvas id="dataAugmentationChart1"></canvas>
                </div>
                <div style="width: 100%;">
                  <canvas id="dataAugmentationChart2"></canvas>
                </div>
              </div>
              <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
                Accuracies (↑) of LIFT/GPT-J with/without data augmentation on
                MNIST. 
              </h2>
              <p class="content">
                Data augmentation means that we are using a noisy version of MNIST
                training data by adding Gaussian noise. Given an MNIST
                image having range [0,1], different levels of noise are added. 
                One can confirm that the data augmentation
                significantly improves the tolerance of LIFT/GPT-J against
                perturbed test data in both Gaussian and signed constant
                noise. 
              </p>
            </div>
          </div>

        </div>
      </div>
  
    </div>
  </div>
</section>

<section class="hero is-small is-light" id="spotlights">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-2">Building on Our Research: Spotlights</h2>
      
      <div class="columns is-centered">
        <div class="column is-10">
          <!-- First point -->
          <div class="box highlight-box">
            <div class="columns is-vcentered">
              <div class="column is-1">
                <span class="icon is-large">
                  <i class="fas fa-vial fa-2x"></i>
                </span>
              </div>
              <div class="column">
                <div class="highlight-content">
                  <span class="highlight-title">Predictive Chemistry</span>
                  <p><b>Leveraging large language models for predictive chemistry.</b>  
                  </p>
                  <p>Kevin Maik Jablonka, Philippe Schwaller, Andres Ortega-Guerrero, Berend Smit <a href="https://www.nature.com/articles/s42256-023-00788-1" style="color: #0171e3; margin-left: 10px;">Learn more</a></p>
                </div>
              </div>
            </div>
          </div>

          <!-- Second point -->
          <div class="box highlight-box">
            <div class="columns is-vcentered">
              <div class="column is-1">
                <span class="icon is-large">
                  <i class="fas fa-atom fa-2x"></i>
                </span>
              </div>
              <div class="column">
                <div class="highlight-content">
                  <span class="highlight-title">Physics of Metamaterial</span>
                  <p><b>Can Large Language Models Learn the Physics of Metamaterials? An Empirical Study with ChatGPT.</b>  
                  </p>
                  <p>Darui Lu, Yang Deng, Jordan M. Malof, Willie J. Padilla <a href="https://arxiv.org/abs/2404.15458" style="color: #0171e3; margin-left: 10px;">Learn more</a></p>
                </div>
              </div>
            </div>
          </div>

          <!-- Third point -->
          <div class="box highlight-box">
            <div class="columns is-vcentered">
              <div class="column is-1">
                <span class="icon is-large has-text-primary">
                  <i class="fas fa-chart-line fa-2x"></i>
                </span>
              </div>
              <div class="column">
                <div class="highlight-content">
                  <span class="highlight-title">Bayesian Inference</span>
                  <p><b>Large Language Models to Enhance Bayesian Optimization.</b>  
                  </p>
                  <p>Tennison Liu*, Nicolás Astorga*, Nabeel Seedat, Mihaela van der Schaar <a href="https://arxiv.org/abs/2402.03921" style="color: #0171e3; margin-left: 10px;">Learn more</a></p>
                </div>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container content">
      <h2 class="title">Cite Us</h2>
      <pre><code>
        @inproceedings{tuan_zeng_2022_lift,
          author = {Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Gira, Michael and Rajput, Shashank and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
          booktitle = {Advances in Neural Information Processing Systems},
          pages = {11763--11784},
          title = {LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks},
          volume = {35},
          year = {2022}
         }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page, and further updated by <a href="https://yzeng58.github.io" target="_blank">Yuchen Zeng</a>.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
