<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks</title>
  <link rel="icon" type="image/x-icon" href="static/images/lift_logo.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script> 

  <!-- Chart.js Library -->
  <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
    <style>
        .container {
            padding: 20px;
        }
        .heatmap-container {
            overflow-x: auto;
            margin: 20px 0;
        }
        .heatmap {
            border-collapse: collapse;
            width: 100%;
            min-width: 800px;
        }
        .heatmap th, .heatmap td {
            padding: 10px;
            text-align: center;
            border: 1px solid #ddd;
            position: relative;
        }
        .heatmap th {
            background-color: #f5f5f5;
            font-weight: bold;
        }
        .heatmap td:first-child {
            text-align: left;
            font-weight: bold;
            background-color: #f5f5f5;
            position: sticky;
            left: 0;
            z-index: 1;
        }
        .tooltip {
            position: absolute;
            background-color: rgba(0, 0, 0, 0.8);
            color: white;
            padding: 8px;
            border-radius: 4px;
            font-size: 14px;
            z-index: 100;
            display: none;
            white-space: nowrap;
        }
        .button-group {
            margin-bottom: 20px;
        }
        .legend {
            display: flex;
            justify-content: center;
            align-items: center;
            margin: 20px 0;
        }
        .legend-gradient {
            width: 200px;
            height: 20px;
            background: linear-gradient(to right, #f7fbff, #084594);
            margin: 0 10px;
        }
    </style>

</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">LIFT: Language-Interfaced Fine-Tuning for Non-Language Machine Learning Tasks</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://tuanqdinh.com/" target="_blank">Tuan Dinh</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="https://yzeng58.github.io" target="_blank">Yuchen Zeng</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="https://ruisu516.github.io/" target="_blank">Ruisu Zhang</a>,</span>
                  <span class="author-block">
                    <a href="https://myhakureimu.github.io/" target="_blank">Ziqian Lin</a>,</span>
                  <span class="author-block">
                    <a href="https://gira.dev/" target="_blank">Michael Gira</a>,</span>
                  <span class="author-block">
                    <a href="https://shashankrajput.github.io/" target="_blank">Shashank Rajput</a>,</span>
                  <span class="author-block">
                    <a href="https://itml.yonsei.ac.kr/professor" target="_blank">Jy-yong Sohn</a>,</span>
                  <span class="author-block">
                    <a href="https://papail.io/" target="_blank">Dimitris Papailiopoulos</a>,</span>
                  <span class="author-block">
                    <a href="https://kangwooklee.com/aboutme/" target="_blank">Kangwook Lee</a></span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">University of Wisconsin-Madison<br>NeurIPS 2022</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Denotes equal contribution. Author order is alphabetical.</small></span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2206.06565" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>


                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/UW-Madison-Lee-Lab/LanguageInterfacedFineTuning" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://neurips.cc/virtual/2022/poster/54500" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="fas fa-video"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- Teaser video-->
<section class="hero teaser" style="margin-top: -3rem;">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="static/videos/lift_demo.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        A high-level illustration of the LIFT framework. LIFT has a two-phase procedure: (1) converting the dataset into sentences and (2) fine-tuning the pretrained LLMs (e.g., GPT) on the obtained sentences. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Fine-tuning pretrained language models (LMs) without making any architectural changes has become a norm for learning various language downstream tasks. However, for non-language downstream tasks, a common practice is to employ task-specific designs for input, output layers, and loss functions. For instance, it is possible to fine-tune an LM into an MNIST classifier by replacing the word embedding layer with an image patch embedding layer, the word token output layer with a 10-way output layer, and the word prediction loss with a 10-way classification loss, respectively. A natural question arises: Can LM fine-tuning solve non-language downstream tasks without changing the model architecture or loss function? To answer this, we propose Language-Interfaced Fine-Tuning (LIFT) and study its efficacy and limitations by conducting an extensive empirical study on a suite of non-language classification and regression tasks. LIFT does not make any changes to the model architecture or loss function, and it solely relies on the natural language interface, enabling "no-code machine learning with LMs." We find that LIFT performs comparably well across a wide range of low-dimensional classification and regression tasks, matching the performances of the best baselines in many cases, especially for the classification tasks. We also report experimental results on the fundamental properties of LIFT, including inductive bias, robustness, and sample complexity. We also analyze the effect of pretraining on LIFT and a few properties/techniques specific to LIFT, e.g., context-aware learning via appropriate prompting, calibrated predictions, data generation, and two-stage fine-tuning. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">One Model, Many Tasks</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/tabular_regression.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          LIFT for <b>Tabular Regression</b>
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/tabular_classification.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          LIFT for <b>Tabular Classification</b>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/image_classification.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          LIFT for <b>Image Classification</b>
       </h2>
     </div>
     <div class="item">
      <!-- Your image here -->
      <img src="static/images/image_generation.jpg" alt="MY ALT TEXT"/>
      <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
        LIFT for <b>Image Generation</b>
     </h2>
   </div>
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Language-Interfaced Learning: Fine-Tuning versus In-Context Learning</h2>
      
      <canvas id="comparisonChart"></canvas>
      
      <!-- Chart.js Library -->
      <script src="https://cdn.jsdelivr.net/npm/chart.js"></script>
      
      <!-- Chart Configuration Script -->
      <script>
        const ctx = document.getElementById('comparisonChart').getContext('2d');
        const comparisonChart = new Chart(ctx, {
          type: 'bar',
          data: {
            labels: ['Customers', 'TAE', 'Vehicle', 'LED', 'Hamster', 'Breast'],
            datasets: [
              {
                label: 'LIICL/Subset',
                data: [60.61, 37.64, 28.82, 8.00, 57.78, 62.07],
                backgroundColor: 'rgba(6,56,120,0.8)',
                borderColor: 'rgba(6,56,120,1)',
                borderWidth: 1
              },
              {
                label: 'LIFT/Subset',
                data: [63.26, 33.33, 23.73, 11.33, 53.33, 70.69],
                backgroundColor: 'rgba(34,113,181,0.8)',
                borderColor: 'rgba(34,113,181,1)',
                borderWidth: 1
              },
              {
                label: 'LIFT/Full-data',
                data: [84.85, 65.59, 70.20, 69.33, 53.33, 71.26],
                backgroundColor: 'rgba(158,202,225,0.8)',
                borderColor: 'rgba(158,202,225,1)',
                borderWidth: 1
              }
            ]
          },
          options: {
            responsive: true,
            plugins: {
              tooltip: {
                mode: 'index',
                intersect: false,
              },
              legend: {
                position: 'top',
              },
              title: {
                display: true,
                text: 'Comparison of Accuracies Between ICL and Fine-Tuning with LIFT on OpenML Datasets'
              }
            },
            scales: {
              x: {
                stacked: false,
                title: {
                  display: true,
                  text: 'Datasets'
                }
              },
              y: {
                beginAtZero: true,
                title: {
                  display: true,
                  text: 'Accuracy (%)'
                }
              }
            }
          }
        });
      </script>
      
      <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
        Comparison of accuracies between ICL and fine-tuning with LIFT on OpenML datasets.
      </h2>
      <p>
        "LIFT/Full-Data" and "LIFT/Subset" represent LIFT on the full dataset and its subset used correspondingly in the ICL setting (number of prompts). Here, the size of the subset is chosen to satisfy the LMs' context length. Overall, LIFT/GPTs on full data achieve the best performances. However, when using the same number of samples, LIFT and ICL are more comparable in most cases. Note that both methods may be worse than MCC due to the limited training data in some cases.
      </p>
    </div>
  </div>
</section>

<section class="hero is-small is-light">
  <div class="hero-body">
      <div class="container is-max-desktop">
          <h2 class="title is-3">Model Performance Comparison Heatmap</h2>
          
          <div class="button-group">
              <button class="button is-info" onclick="filterData('all')">All Data</button>
              <button class="button is-info" onclick="filterData('synthetic')">Synthetic Data</button>
              <button class="button is-info" onclick="filterData('tabular')">Tabular Data</button>
          </div>

          <div class="heatmap-container" id="heatmapContainer">
              <!-- Heatmap will be inserted here -->
          </div>
          
          <div class="legend">
              <span>0%</span>
              <div class="legend-gradient"></div>
              <span>100%</span>
          </div>

          <h2 class="subtitle has-text-centered">
              Heatmap of model performances across different datasets
          </h2>
          <p class="content">
              The heatmap shows performance comparisons between different models across various datasets. 
              Darker blue indicates higher accuracy. Hover over cells to see exact accuracy values and standard deviations.
          </p>
      </div>
  </div>
</section>

<script>
  // Data definition
  const syntheticData = [
      { dataset: 'circles', p_c: '2/2', MCC: 50.00, LogReg: [48.58, 1.94], DT: [77.42, 0.24], RBF_SVM: [83.08, 0.59], XG: [81.42, 0.31], LIFT_GPT_J: [79.95, 1.53], LIFT_GPT_2: [81.17, 0.42] },
      { dataset: 'two circles', p_c: '2/2', MCC: 50.00, LogReg: [49.83, 4.18], DT: [75.50, 0.20], RBF_SVM: [80.00, 0.54], XG: [79.25, 0.35], LIFT_GPT_J: [75.92, 1.65], LIFT_GPT_2: [81.42, 0.82] },
      { dataset: 'blobs', p_c: '2/4', MCC: 25.00, LogReg: [96.75, 0.00], DT: [96.08, 0.82], RBF_SVM: [96.75, 0.00], XG: [96.17, 0.12], LIFT_GPT_J: [96.17, 0.59], LIFT_GPT_2: [96.67, 0.24] },
      { dataset: 'moons', p_c: '2/4', MCC: 50.00, LogReg: [88.58, 0.12], DT: [99.25, 0.41], RBF_SVM: [100.00, 0.00], XG: [99.83, 0.12], LIFT_GPT_J: [99.58, 0.42], LIFT_GPT_2: [100.00, 0.00] },
      { dataset: '9Clusters', p_c: '2/9', MCC: 11.25, LogReg: [100.00, 0.00], DT: [100.00, 0.00], RBF_SVM: [100.00, 0.00], XG: [100.00, 0.00], LIFT_GPT_J: [99.75, 0.00], LIFT_GPT_2: [100.00, 0.00] }
  ];

  const tabularData = [
      { dataset: 'Customers', p_c: '8/2', MCC: 68.18, LogReg: [87.12, 0.54], DT: [85.98, 0.53], RBF_SVM: [86.36, 0.00], XG: [85.23, 0.00], LIFT_GPT_J: [85.23, 1.61], LIFT_GPT_2: [84.85, 1.42] },
      { dataset: 'Pollution', p_c: '15/2', MCC: 50.00, LogReg: [58.33, 11.79], DT: [77.78, 3.93], RBF_SVM: [58.33, 6.81], XG: [63.89, 7.86], LIFT_GPT_J: [63.89, 3.93], LIFT_GPT_2: [63.89, 7.86] },
      { dataset: 'Spambase', p_c: '57/2', MCC: 60.59, LogReg: [93.27, 0.00], DT: [90.70, 0.14], RBF_SVM: [93.70, 0.00], XG: [95.87, 0.00], LIFT_GPT_J: [94.03, 0.54], LIFT_GPT_2: [94.90, 0.36] },
      { dataset: 'Hill-Valley', p_c: '100/2', MCC: 49.79, LogReg: [77.78, 0.00], DT: [56.38, 0.89], RBF_SVM: [68.72, 0.00], XG: [59.26, 0.00], LIFT_GPT_J: [100.00, 0.20], LIFT_GPT_2: [99.73, 0.19] },
      { dataset: 'IRIS', p_c: '4/3', MCC: 33.33, LogReg: [96.67, 0.00], DT: [97.77, 3.85], RBF_SVM: [100.00, 0.00], XG: [100.00, 0.00], LIFT_GPT_J: [96.67, 0.00], LIFT_GPT_2: [97.00, 0.00] },
      { dataset: 'TAE', p_c: '5/3', MCC: 35.48, LogReg: [45.16, 4.56], DT: [65.59, 5.49], RBF_SVM: [53.76, 6.63], XG: [66.67, 8.05], LIFT_GPT_J: [61.29, 6.97], LIFT_GPT_2: [65.59, 6.63] },
      { dataset: 'CMC', p_c: '9/3', MCC: 42.71, LogReg: [49.49, 0.83], DT: [56.72, 0.32], RBF_SVM: [56.50, 0.97], XG: [52.43, 0.42], LIFT_GPT_J: [49.83, 0.28], LIFT_GPT_2: [57.74, 0.89] },
      { dataset: 'Wine', p_c: '13/3', MCC: 38.89, LogReg: [100.00, 0.00], DT: [93.52, 2.62], RBF_SVM: [100.00, 0.00], XG: [97.22, 0.00], LIFT_GPT_J: [93.52, 1.31], LIFT_GPT_2: [92.59, 1.31] },
      { dataset: 'Vehicle', p_c: '18/4', MCC: 25.88, LogReg: [80.39, 1.00], DT: [63.92, 2.37], RBF_SVM: [81.18, 0.48], XG: [73.14, 0.28], LIFT_GPT_J: [64.31, 2.37], LIFT_GPT_2: [70.20, 2.73] },
      { dataset: 'LED', p_c: '7/10', MCC: 11.00, LogReg: [68.67, 0.94], DT: [66.33, 2.87], RBF_SVM: [68.00, 0.82], XG: [66.00, 0.82], LIFT_GPT_J: [65.33, 0.47], LIFT_GPT_2: [69.33, 2.05] },
      { dataset: 'OPT', p_c: '64/10', MCC: 10.14, LogReg: [96.53, 0.22], DT: [89.80, 1.09], RBF_SVM: [97.95, 0.00], XG: [97.48, 0.17], LIFT_GPT_J: [98.22, 0.11], LIFT_GPT_2: [98.99, 0.30] },
      { dataset: 'Mfeat', p_c: '216/10', MCC: 10.00, LogReg: [97.67, 0.12], DT: [87.67, 1.05], RBF_SVM: [98.83, 0.24], XG: [96.75, 0.00], LIFT_GPT_J: [94.17, 1.75], LIFT_GPT_2: [93.08, 0.24] },
      { dataset: 'Margin', p_c: '64/100', MCC: 0.94, LogReg: [81.35, 0.15], DT: [43.86, 1.21], RBF_SVM: [81.98, 0.30], XG: [70.21, 0.29], LIFT_GPT_J: [50.23, 1.33], LIFT_GPT_2: [59.37, 0.92] },
      { dataset: 'Texture', p_c: '64/100', MCC: 0.94, LogReg: [81.67, 0.97], DT: [46.88, 1.93], RBF_SVM: [83.44, 0.89], XG: [70.73, 1.41], LIFT_GPT_J: [50.32, 2.18], LIFT_GPT_2: [67.50, 1.42] }
  ];

  function getColor(value) {
      // Returns a blue color scale from light to dark based on value
      const minValue = 0;
      const maxValue = 100;
      const normalizedValue = (value - minValue) / (maxValue - minValue);
      
      // Using the Blues color scheme
      const colors = [
          [247, 251, 255],
          [222, 235, 247],
          [198, 219, 239],
          [158, 202, 225],
          [107, 174, 214],
          [66, 146, 198],
          [33, 113, 181],
          [8, 69, 148]
      ];
      
      const index = Math.floor(normalizedValue * (colors.length - 1));
      const color = colors[Math.min(index, colors.length - 1)];
      return `rgb(${color[0]}, ${color[1]}, ${color[2]})`;
  }

  function createTooltip(cell, dataset, method, value, std) {
      const tooltip = document.createElement('div');
      tooltip.className = 'tooltip';
      tooltip.innerHTML = `
          Dataset: ${dataset}<br>
          Method: ${method}<br>
          Accuracy: ${value.toFixed(2)}% ±${std.toFixed(2)}
      `;
      cell.appendChild(tooltip);

      cell.addEventListener('mouseover', () => {
          tooltip.style.display = 'block';
      });

      cell.addEventListener('mouseout', () => {
          tooltip.style.display = 'none';
      });
  }

  function createHeatmap(data) {
      const container = document.getElementById('heatmapContainer');
      container.innerHTML = ''; // Clear previous content

      const table = document.createElement('table');
      table.className = 'heatmap';

      // Create header row
      const methods = ['LogReg', 'DT', 'RBF-SVM', 'XG', 'LIFT/GPT-J', 'LIFT/GPT-2'];
      const headerRow = document.createElement('tr');
      headerRow.innerHTML = '<th>Dataset</th>' + 
          methods.map(method => `<th>${method}</th>`).join('');
      table.appendChild(headerRow);

      // Create data rows
      data.forEach(row => {
          const tr = document.createElement('tr');
          tr.innerHTML = `<td>${row.dataset}</td>`;
          
          methods.forEach(method => {
              const methodKey = method.replace('-', '_').replace('/', '_');
              const [value, std] = row[methodKey];
              const td = document.createElement('td');
              td.style.backgroundColor = getColor(value);
              td.textContent = value.toFixed(1);
              createTooltip(td, row.dataset, method, value, std);
              tr.appendChild(td);
          });

          table.appendChild(tr);
      });

      container.appendChild(table);
  }

  function filterData(type) {
      let data;
      switch(type) {
          case 'synthetic':
              data = syntheticData;
              break;
          case 'tabular':
              data = tabularData;
              break;
          default:
              data = [...syntheticData, ...tabularData];
      }
      createHeatmap(data);
  }

  // Initial heatmap creation
  document.addEventListener('DOMContentLoaded', function() {
      filterData('all');
  });
</script>



<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
      <h2 class="title is-3">Building on Our Research: Spotlights</h2>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
        <img src="static/images/nature_small.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          Nature Machine Intelligence paper on GPT-3 fine-tuning for complex chemistry problems using LIFT
        </h2>
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/iclr.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          ICLR Paper integrating LLMs with Bayesian Optimization for hyperparameter tuning using LIFT
      </div>
      <div class="item">
        <!-- Your image here -->
        <img src="static/images/bolift.jpg" alt="MY ALT TEXT"/>
        <h2 class="subtitle has-text-centered" style="margin-top: 1rem;">
          BO-LIFT: in-context learning with LLMs for Bayesian Optimization of catalysts
       </h2>
     </div>
  </div>
</div>
</div>
</section>


<!-- Paper poster -->
<section class="hero is-small is-light">
  <div class="hero-body">
    <div class="container">
      <h2 class="title">Poster</h2>

      <iframe  src="static/pdfs/lift_poster.pdf" width="100%" height="550">
          </iframe>
        
      </div>
    </div>
  </section>
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @inproceedings{tuan_zeng_2022_lift,
          author = {Dinh, Tuan and Zeng, Yuchen and Zhang, Ruisu and Lin, Ziqian and Gira, Michael and Rajput, Shashank and Sohn, Jy-yong and Papailiopoulos, Dimitris and Lee, Kangwook},
          booktitle = {Advances in Neural Information Processing Systems},
          pages = {11763--11784},
          title = {LIFT: Language-Interfaced Fine-Tuning for Non-language Machine Learning Tasks},
          volume = {35},
          year = {2022}
         }
      </code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
